{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad843e3-ac0b-4266-9eba-52eba01d739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 22:40:07,147 : INFO : collecting all words and their counts\n",
      "2025-04-02 22:40:07,148 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-02 22:40:07,200 : INFO : collected 12099 word types from a corpus of 188848 raw words and 10000 sentences\n",
      "2025-04-02 22:40:07,200 : INFO : Creating a fresh vocabulary\n",
      "2025-04-02 22:40:07,222 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4028 unique words (33.29% of original 12099, drops 8071)', 'datetime': '2025-04-02T22:40:07.222343', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-02 22:40:07,223 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 179103 word corpus (94.84% of original 188848, drops 9745)', 'datetime': '2025-04-02T22:40:07.223466', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-02 22:40:07,255 : INFO : deleting the raw counts dictionary of 12099 items\n",
      "2025-04-02 22:40:07,257 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2025-04-02 22:40:07,257 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 127397.55238210877 word corpus (71.1%% of prior 179103)', 'datetime': '2025-04-02T22:40:07.257070', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-02 22:40:07,308 : INFO : estimated required memory for 4028 words and 300 dimensions: 11681200 bytes\n",
      "2025-04-02 22:40:07,309 : INFO : resetting layer weights\n",
      "2025-04-02 22:40:07,322 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-02T22:40:07.322283', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-04-02 22:40:07,323 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 4028 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-02T22:40:07.322283', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-04-02 22:40:07,870 : INFO : EPOCH 0: training on 188848 raw words (127327 effective words) took 0.5s, 236078 effective words/s\n",
      "2025-04-02 22:40:08,524 : INFO : EPOCH 1: training on 188848 raw words (127457 effective words) took 0.6s, 197067 effective words/s\n",
      "2025-04-02 22:40:09,140 : INFO : EPOCH 2: training on 188848 raw words (127454 effective words) took 0.6s, 210028 effective words/s\n",
      "2025-04-02 22:40:09,770 : INFO : EPOCH 3: training on 188848 raw words (127390 effective words) took 0.6s, 204406 effective words/s\n",
      "2025-04-02 22:40:10,394 : INFO : EPOCH 4: training on 188848 raw words (127308 effective words) took 0.6s, 206433 effective words/s\n",
      "2025-04-02 22:40:10,395 : INFO : Word2Vec lifecycle event {'msg': 'training on 944240 raw words (636936 effective words) took 3.1s, 207337 effective words/s', 'datetime': '2025-04-02T22:40:10.395339', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-04-02 22:40:10,396 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4028, vector_size=300, alpha=0.025>', 'datetime': '2025-04-02T22:40:10.396395', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-04-02 22:40:10,401 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_skipgram.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-04-02T22:40:10.401992', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-04-02 22:40:10,403 : INFO : not storing attribute cum_table\n",
      "2025-04-02 22:40:10,417 : INFO : saved word2vec_skipgram.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 模型参数 =====\n",
      "模型架构: Skip-Gram\n",
      "词表大小: 4028\n",
      "训练总词数: 188848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 词向量训练（Skip-Gram模式）\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import logging  # 添加日志记录\n",
    "\n",
    "# 配置日志输出\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 1. 数据预处理\n",
    "def preprocess_text(text):\n",
    "    \"\"\"文本清洗和分词处理\"\"\"\n",
    "    # 去除标点符号（扩展更全的标点集合）\n",
    "    punctuation = \"，。！？、；：“”‘’【】（）《》~@#￥%……&*\"\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p, \"\")\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "# 读入训练集文件\n",
    "data = pd.read_csv('train.csv')\n",
    "corpus = [preprocess_text(str(comment)) for comment in data['comment'].values]\n",
    "\n",
    "# 2. Skip-Gram模型训练\n",
    "model = Word2Vec(\n",
    "    corpus,\n",
    "    sg=1,  # 关键修改：sg=1表示使用Skip-Gram（默认CBOW是sg=0）\n",
    "    vector_size=300,  # 词向量维度\n",
    "    window=5,        # 上下文窗口大小（Skip-Gram通常用更大窗口）\n",
    "    min_count=3,     # 忽略低频词\n",
    "    workers=4,       # 并行线程数\n",
    "    negative=5,      # 负采样数（Skip-Gram推荐5-20）\n",
    "    hs=0,            # 禁用层次softmax（与negative采样二选一）\n",
    "    alpha=0.025,     # 初始学习率\n",
    "    min_alpha=0.0001 # 最小学习率\n",
    ")\n",
    "\n",
    "# 3. 模型保存与加载\n",
    "model.save(\"word2vec_skipgram.model\")  # 保存模型\n",
    "# model = Word2Vec.load(\"word2vec_skipgram.model\")  # 加载模型\n",
    "\n",
    "# 4. 模型验证\n",
    "print('\\n===== 模型参数 =====')\n",
    "print(f\"模型架构: {'Skip-Gram' if model.sg else 'CBOW'}\")\n",
    "print(f\"词表大小: {len(model.wv)}\")\n",
    "print(f\"训练总词数: {model.corpus_total_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8988a179-58d7-4c86-bfbb-d39c95ebdf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与'点赞'最相似的词：[('传说', 0.989485502243042), ('大方', 0.9877336025238037), ('送货上门', 0.9873219132423401)]\n",
      "与'不错'最相似的词：[('挺不错', 0.9070080518722534), ('很棒', 0.906531035900116), ('好极了', 0.9012317061424255)]\n",
      "与'难吃'最相似的词：[('咸', 0.875076174736023), ('垃圾', 0.8659026026725769), ('实在', 0.8470757007598877)]\n",
      "与'推荐'最相似的词：[('值得', 0.9029862880706787), ('一试', 0.8813337087631226), ('一去', 0.8808634877204895)]\n",
      "与'地道'最相似的词：[('正', 0.976787269115448), ('很赞', 0.9732523560523987), ('大爱', 0.96816486120224)]\n",
      "\n",
      "'地道'的词向量（前10维）:\n",
      "[-0.01213648  0.10693915  0.00260235  0.06403901 -0.04777374 -0.11223552\n",
      "  0.15519957  0.52974755 -0.07509932 -0.16252017]\n"
     ]
    }
   ],
   "source": [
    "# 语义相似度查询\n",
    "test_words = ['点赞', '不错', '难吃', '推荐', '地道']\n",
    "for word in test_words:\n",
    "    if word in model.wv:\n",
    "        print(f\"与'{word}'最相似的词：{model.wv.most_similar(word, topn=3)}\")\n",
    "\n",
    "# 向量获取示例\n",
    "if '地道' in model.wv:\n",
    "    print(f\"\\n'地道'的词向量（前10维）:\\n{model.wv['地道'][:10]}\")\n",
    "else:\n",
    "    print(\"\\n警告：'地道'不在词表中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47c61cc-8710-4eee-b846-2f81be9d6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'环境'的词向量（前5维）:\n",
      "[-0.06844943  0.20632029 -0.07643649  0.04566484  0.08594491]\n",
      "词向量形状: (300,)\n"
     ]
    }
   ],
   "source": [
    "# 检查并输出\"环境\"的词向量及形状\n",
    "if '环境' in model.wv:\n",
    "    env_vector = model.wv['环境']\n",
    "    print(f\"'环境'的词向量（前5维）:\\n{env_vector[:5]}\")\n",
    "    print(f\"词向量形状: {env_vector.shape}\")  # 应输出 (300,)\n",
    "else:\n",
    "    print(\"警告：'环境'不在词表中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a88d47-f5b1-4ac3-ab25-f9a6d92154a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "与'好吃'最相似的3个词:\n",
      "棒: 0.8207\n",
      "入味: 0.8146\n",
      "香: 0.8118\n",
      "\n",
      "词语相似度:\n",
      "'好吃' vs '美味': 0.7766\n",
      "'好吃' vs '蟑螂': 0.3039\n"
     ]
    }
   ],
   "source": [
    "# 输出与\"好吃\"最相似的3个词\n",
    "if '好吃' in model.wv:\n",
    "    print(\"\\n与'好吃'最相似的3个词:\")\n",
    "    for word, similarity in model.wv.most_similar('好吃', topn=3):\n",
    "        print(f\"{word}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"警告：'好吃'不在词表中\")\n",
    "\n",
    "# 计算词语相似度\n",
    "similarity_results = []\n",
    "for word in ['美味', '蟑螂']:\n",
    "    if '好吃' in model.wv and word in model.wv:\n",
    "        sim = model.wv.similarity('好吃', word)\n",
    "        similarity_results.append((word, sim))\n",
    "    else:\n",
    "        print(f\"警告：'{word}'不在词表中\")\n",
    "\n",
    "print(\"\\n词语相似度:\")\n",
    "for word, sim in similarity_results:\n",
    "    print(f\"'好吃' vs '{word}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9eceec0-25f5-4f19-bf5b-bea9dc9e2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "向量运算 '餐厅 + 聚会 - 安静' ≈ '部门' (相似度: 0.9451)\n"
     ]
    }
   ],
   "source": [
    "# 向量类比计算\n",
    "if all(word in model.wv for word in ['餐厅', '聚会', '安静']):\n",
    "    result = model.wv.most_similar(\n",
    "        positive=['餐厅', '聚会'],\n",
    "        negative=['安静'],\n",
    "        topn=1\n",
    "    )\n",
    "    print(f\"\\n向量运算 '餐厅 + 聚会 - 安静' ≈ '{result[0][0]}' (相似度: {result[0][1]:.4f})\")\n",
    "else:\n",
    "    print(\"警告：计算所需的词未全部存在于词表中\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
